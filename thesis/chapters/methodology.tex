The methodology of this project is designed to provide a rigorous framework for investigating the application of federated learning combined with autoencoders for anomaly-based DDoS detection. Given the complexity of the problem domain, which intersects machine learning, distributed computing, and cybersecurity, it is necessary to adopt a well-structured approach that balances research design with practical experimentation. This chapter outlines the methodological choices that support the study, including research philosophy, dataset selection, data statistical analysis methods, data preprocessing and partitioning strategies, model and algorithm selection, the overall experimental design, the evaluation approaches with appropriate metrics, and project management aspects.

At its core, this project adopts a quantitative research approach, as it aligns most effectively with the research aims and objectives, which center on measuring, comparing, and evaluating model performance under varying training setups. Quantitative research provides the tools necessary to analyze anomaly detection and federated learning systems through objective, measurable outcomes, such as accuracy, precision, recall, convergence behavior, and computational efficiency. This approach allows the study to produce reproducible results, benchmarked against established statistical measures, thereby ensuring methodological rigor and comparability. While qualitative methods, such as expert interviews or case studies, could potentially offer valuable insights into deployment challenges in operational environments, they fall outside the scope of this project. Instead, the methodology is deliberately constrained to controlled, data-driven experimentation in order to generate findings that are both systematic and verifiable.

To support this quantitative framework, the project works on two well-known benchmark datasets developed by the Canadian Institute for Cybersecurity (CIC), which are CIC-IDS2017 \citep{Sharafaldin2018TowardGA,cicids2017} and CIC-DDoS2019 \citep{8888419,cicddos2019}. These datasets are widely used in the field of network intrusion detection systems (IDS) due to their rich, labeled traffic data, realistic simulation of attack scenarios, and comprehensive coverage of modern cyber threats. Both datasets provide a reliable foundation for evaluating machine learning models designed to detect anomalous or malicious activity within large-scale network environments. The CIC-IDS2017 dataset was designed to reflect most updated intrusion patterns across various attack surfaces. It contains traffic data corresponding to a wide range of common network-based attacks, including Brute Force FTP, Brute Force SSH, DoS (Denial of Service), Heartbleed, Web Attacks, Infiltration, Botnet, and DDoS (Distributed Denial of Service) \citep{cicids2017}. The dataset was generated in a controlled environment using realistic user behavior simulations, allowing the resulting flows to mimic the structure and variability of production-level traffic. The benign traffic in CIC-IDS2017 was generated using B-Profile, a profiling system that simulates user activities based on predefined usage patterns and time-based distributions \citep{Sharafaldin2018TowardGA}. This ensures that the dataset includes realistic background traffic, which is essential for training anomaly-based models that aim to distinguish between normal and abnormal behavior. In contrast, the CIC-DDoS2019 dataset focuses specifically on Distributed Denial of Service (DDoS) attacks, which are among the most prevalent and damaging forms of cyberattacks. This dataset includes a wide variety of modern attacks, which were executed using both TCP and UDP protocols. The attack types include MSSQL, SSDP, CharGen, NTP, TFTP, DNS, LDAP, NETBIOS, and SNMP \citep{cicddos2019}. These attacks were proceeded from a separate network to emulate distributed attack vectors, thereby increasing the validity of the dataset for real-world DDoS detection research. Similar to CIC-IDS2017, the benign traffic in CIC-DDoS2019 was also generated using the B-Profile system, ensuring consistency in traffic generation methodologies across both datasets. Both datasets are structured as flow-based records, rather than raw packet-level captures. This is accomplished using CICFlowMeter \citep{cicflowmeter}, a tool that converts packet captures (PCAP files) into flow summaries. Together, these datasets form the empirical foundation of the study, enabling a comprehensive evaluation of federated learning combined with autoencoder-based anomaly detection in heterogeneous environments.

Building upon the discussion in the introduction and literature review, this study adopts autoencoders as the core model for anomaly detection. Autoencoders are unsupervised models and do not require labeled data to train. Autoencoders also have a relatively simple architecture as they are just plain neural networks often having a few layers, which helps autoencoders cost less computational efforts. These two points make autoencoders outweigh supervised models like CNN and RNN, which require labeled data and have more complex architecture. In addition, autoencoders' neural network structure aligns well with the federated learning, unlike traditional threshold-based or clustering-based approaches that are less compatible with distributed training. This research focuses only on the traditional autoencoders without assessing advanced variants like VAE and AAE. While these alternatives may offer potential benefits, there is insufficient evidence of consistent performance improvements in this anomaly detection context, and their inclusion would introduce unnecessary complexity. Moreover, limiting the scope to traditional autoencoders ensures that the study stays within the given time constraints.

The raw datasets undergo a series of preprocessing and analysis steps to ensure quality and suitability for model training. Initially, missing values and duplicate entries are removed (in cases where missing values are sparse, they may be inferred to retain sufficient data), which avoids models training on failed data. As this project follows the anomaly detection approach, all categories of attack traffic are consolidated under the single label of \textbf{"anomalous"}, while normal traffic is retained as \textbf{"benign"}. To evaluate statistical differences between benign and anomalous samples, inferential tests, such as the two-sample t-test or the Mann–Whitney U test, are applied to each feature, which confirm datasets suitable to train with autoencoders for anomaly detection. The dataset is then resampled and split into one benign training set, one benign testing set and one anomalous testing set, which are based on the hardware availability and the benign-to-anomalous ratio. For each numerical feature, descriptive statistics such as mean, standard deviation, maximum, and minimum are computed, and histograms are plotted to assess their distributions. The observed distributions further guide normalization (e.g., log-transforming features following a power-law distribution or z-scaling features following a normal distribution), and features are subsequently scaled to a uniform [0, 1] range using min–max scaling if necessary. This step help models be trained faster and attend to each feature equally. Finally, categorical features are encoded using one-hot encoding to make them suitable for model input.

After the preprocessing process, autoencoders are trained on the benign training set to learn benign traffic patterns and from these be able to indicate abnormalities. Training processes apply weight decay and early stopping to avoid overfitting. Random search, which randomly selects combinations of parameters, is employed in training to find the optimal hyperparameters (e.g., learning rate, batch size, and weight decay). Loss values during training are recorded to investigate the convergence behavior. Performance is evaluated on the two testing sets with metrics, including:

\begin{itemize}
    \item \textbf{Precision:} the fraction of normal traffic correctly classified as benign.
    \item \textbf{True positive rate (Recall/TPR):} the fraction of DDoS traffic correctly classified as anomalous.
    \item \textbf{False positive rate (FPR):} the fraction of normal traffic incorrectly classified as anomalous.
    \item \textbf{Accuracy:} the fraction of traffic (both normal and DDoS traffic) correctly classified.
    \item \textbf{F1-score:} a metric, calculated by $F_{1} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$, balances precision and recall, and is more reliable than accuracy in imbalanced datasets.
    \item \textbf{Area under the ROC curve (AUC):} the ROC curve represents TRP and FRP among thresholds, which has an under-area (AUC) measuring the model’s ability to classify benign and anomalous traffic.
\end{itemize}

Note that the threshold selection method, which decides the reconstruction error value separating benign and anomalous data, significantly affects the model performance and detection ability. Lower thresholds increase the detection rate (recall) but also raise false positives (FPR), while higher thresholds reduce false alarms (increase precision) but face the risk of missing attacks. In this study, the threshold yielding the highest F1-score is chosen for evaluation and comparison, which balances the importance of benign and anomalous detection. All listed metrics and the proposed threshold selection method are derived from labeled datasets, so in real-world deployments where labeled data is often in shortage, thresholds can be set according to operational priorities (e.g., maximizing detection rate vs. minimizing false alarms) or adjusted adaptively based on system load. 

For federated training, the dataset must be partitioned and distributed across clients. In this study, partitioning is performed based on the source IP addresses of traffic flows. Two primary strategies are employed. The IID distribution divides samples of each IP address evenly across clients, ensuring that every client receives roughly the same number of samples per IP address and an equal total dataset size. By contrast, the Dirichlet distribution ($\text{Dir}(\alpha)$) fragments the samples of each IP address into uneven proportions, which are then allocated to clients. This produces clients with varying sample sizes and different class proportions. The Dirichlet distribution provides a concentration parameter $\alpha$, which allows control over how skewed the data allocations are (a high $\alpha$ leads to more uniform splits, while a low $\alpha$ results in highly imbalanced distributions). Partitioning can also be done with an optional approach, which treats each source IP address or IP range as a separate client, mimicking clients having different geographical attributes. Together, these methods, combined with varying the number of clients, produce different levels of heterogeneity, allowing the robustness of the training process to be evaluated under diverse conditions.

After preparing data, autoencoders are trained under federated settings, beginning with the standard FedAvg algorithm, which serves as the benchmark baseline. Training experiments are conducted in a simulation environment with multiple data partitioning methods and varying numbers of clients. Evaluation steps from centralized training are still applied to evaluate the model performance and convergence behavior. Random search is also employed to find the optimal hyperparameters, such as FedProx's proximal term and FedOptim's server learning rate. Following the FedAvg, models are trained with FedProx to mitigate client drift effects. Next, federated optimization algorithms (FedAdagrad, FedAdam, and FedYogi) are explored to improve convergence speed. Since these algorithms share similar mechanisms of utilizing momentums and second-order moments, they are first compared against each other, and the best-performing one is selected for the remaining experiments. This reduces computational cost while keeping the study focused. Finally, federated optimization algorithms are tried to combine together to reach further gains. Given the large combination of algorithms, partitioning strategies, and client counts, the experiments generate a vast number of performance metrics and training traces. To present the results clearly, only F1-score and accuracy are retained for evaluation and comparison, as they remain broadly informative and also are commonly used in prior research. Additionally, inferential statistical tests (e.g., two-sample t-test and Mann–Whitney U test) can be applied to compare performance between federated optimization algorithms and validate observed differences.

While the proposed methodology is designed to provide a rigorous evaluation of federated learning for anomaly-based DDoS detection, several limitations should be recognized. First, the performance metrics, such as accuracy, precision, recall, and F1-score, were computed on balanced and downsampled datasets to ensure fair comparisons across models. However, this preprocessing reduces the class imbalance inherent in real DDoS traffic, which means the reported metrics may not fully reflect the practical cost of misclassification. In particular, false negatives, instances where attacks are missed, can carry severe operational consequences that extend beyond what these aggregate metrics can capture. Second, the process of selecting anomaly detection thresholds relies on labeled benchmark datasets. While this results from controlled experimentation, it does not perfectly represent deployment conditions where labels are scarce or unavailable. As such, the reliance on labeled data may introduce bias in evaluating detection performance and may not generalize seamlessly to real-world scenarios. Finally, the experiments are conducted in a simulated laboratory environment, which allows controlled investigate performance of algorithms but does not fully capture operational challenges. Factors such as communication overhead, network latency, client dropouts, and large-scale system heterogeneity are not explicitly modeled in this study.

To ensure timely progress and effective execution of the dissertation, a structured project management plan was established and visualized using a Gantt chart (Figure \ref{fig:gantt_chart}). The plan spans from early July to the end of August, which includes six main stages: literature review, architecture design, framework implementation, model training, analysis, and dissertation write-up. Tasks are arranged sequentially but with some overlap to allow flexibility. The plan ensures that core experiments (e.g., FedAvg, FedProx) are prioritized, with buffer time allocated for analysis and final writing. This provides a realistic and manageable schedule leading to timely submission.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/gantt-chart.png}
    \caption{Project's Gantt Chart.}
    \label{fig:gantt_chart}
\end{figure}

In summary, the methodology integrates dataset selection, data preprocessing, model design, and experimental evaluation into a coherent framework for anomaly-based DDoS detection. Starting with CIC-IDS2017 and CIC-DDoS2019, traffic records are cleaned, relabeled, and normalized before being split into training and testing subsets. Autoencoder architectures are developed to model benign traffic, first evaluated in centralized settings and subsequently extended to federated learning with optimization algorithms such as FedAvg, FedProx, and FedAdam. The entire process, from dataset preparation to model training and federated evaluation, is aligned with the project objectives and follows the timeline outlined in the project management plan, ensuring a structured and reproducible workflow.
