\section{Distributed Denial of Service Attacks}

As mentioned earlier, a denial of service (DoS) attack is characterized by an explicit attempt by an attacker to flood a network with malicious traffic and thus reduce legitimate users' bandwidth, prevent access to a service, or disrupt service to a specific system or a user \citep{886455}. A distributed denial of service (DDoS) attack is a more sophisticated variant in which the malicious traffic originates from numerous geographically distributed devices rather than a single machine. With the introduction of user-friendly Internet services such as the World Wide Web, the Internet entered an explosive growth phase in the 1990s with over 6.6 million computers by July 1995 \citep{GLOWNIAK1998135}. Following this growing trend, the first DDoS attacks were then soon documented \citep{9404833}. In 1996, the first notable DDoS attack was recorded by The New York Times \citep{nyt-ddos}. In retaliation for the installation of a new spam email filter, the attacker executed a SYN (flag to request a connection) flood attack on The New York Times' systems, which effectively prevented real requests by legitimate customers and was acknowledged as an unsolvable problem at that time \citep{nyt-ddos}. DDoS attacks are often considered simple to execute but difficult to defend against due to their distributed nature, which hides the true origin of the malicious traffic from being traced. For example, in 2000, a 15-year-old high school student from Canada, known as Mafiaboy, launched a major attack on Yahoo, Amazon, Dell, eBay, CNN, and others, which caused an estimated \$1.5 billion in damage to the global economy \citep{9404833}. This attack action was only stopped after the legal intervention with his arrest \citep{mafiaboy}, illustrating the low barrier of entry to launch a DDoS attack and the potential of severe consequences. As noted in \cite{9404833}, while the original design of the internet offered many advantages over alternative technologies such as X.25, DECNet, and token ring, todayâ€™s reliance on shared infrastructure (e.g., cloud computing, content delivery networks) has shifted the balance between usability and security and created new risks that demand reconsideration of its resilience.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ddostaxonomy.png}
    \caption{DDoS Taxonomy \citep{8888419}.}
    \label{fig:ddos_taxonomies}
\end{figure}

In a recent study by \cite{8888419}, DDoS attacks are categorized into two types, which are reflection-based and exploitation-based DDoS attacks. Reflection-based attacks are distinguished by large traffic volume, which usually involves reflector servers, such as DNS servers with the DNS Amplification attacks (Figure \ref{fig:amplification_ddos_example}). In these attacks, attackers send requests with the target victim's IP address to reflector servers, and the victim is then overwhelmed with response packages. Exploitation-based attacks, on the other hand, exploit protocol design weaknesses or vulnerabilities, which overwhelm the victim's system with unintentional behaviors or undesirable states. The typical examples of this type are the SYN flood attack, which exploits the TCP handshake process and constantly requests open connections to the victim's servers, like the New York Times case \citep{nyt-ddos}, or the newer method, the HTTP/2 Rapid Reset attack used in the Google incident, which sends an HTTP/2 frame named RST\_STREAM after sending a request frame, leading to server exhaustion due to handling RST\_STREAM frames \citep{google-http2-rapid-reset}. A broader taxonomy of DDoS attack vectors is provided in Figure \ref{fig:ddos_taxonomies}, which formed a foundation for the CIC-DDoS2019 dataset used in this project.

\section{Anomaly Detection with Autoencoders}

Anomaly detection has been a subject of research for decades, aiming to identify patterns in data that deviate from expected behavior \citep{9439459}. With the rapid advancement of machine learning, interest in this field has grown significantly. A survey by \cite{9439459} reported 290 research articles on the intersection of machine learning and anomaly detection between 2000 and 2020, with network anomaly detection accounting for 66 of them ($\approx23\%$). This demonstrates a strong trend toward applying machine learning techniques to detect anomalies in computer networks. According to \cite{6524462}, network anomalies can be broadly categorized into two types: performance-related and security-related anomalies, which align with the DDoS categories discussed earlier. Performance-related anomalies are often signaled by excessive traffic volume, leading to degradation in service quality. Security-related anomalies, on the other hand, are typically caused by deliberate exploitation of vulnerabilities to disrupt or damage systems. The study by \cite{6524462} also outlined six approaches for tackling anomaly detection. (1) Statistical methods assume that network traffic follows a certain statistical distribution, which can be either parametric or non-parametric, and apply inference tests to determine whether new observations conform to this distribution. While these methods can provide accurate notification, it is difficult to choose appropriate distributions and tune related parameters, and the assumption of statistical models is sometimes not realistic. (2) Knowledge-based methods operate on predefined rules or patterns of attacks. They can achieve high detection accuracy for known threats but are heavily dependent on expert knowledge, which is often difficult and time-consuming to develop. (3) Classification-based methods, including decision trees, random forests, and support vector machines (SVM), train models to assign traffic to specific classes, which can result in high anomaly detection rates usually surpassing unsupervised methods (e.g., clustering-based methods). However, these techniques rely excessively on hand-labeled datasets and generally fail to identify unknown attacks. (4) Clustering and outlier-based methods, such as k-means clustering, group data into clusters and treat points outside these clusters as anomalies. While they are considered computationally efficient, which is advantageous in the case of large datasets, they can only handle numeric features, leading to less competitive accuracy compared to more advanced techniques. (5) Soft computing methods, particularly neural network approaches, learn informative features directly from data to identify anomalies. They offer benefits such as incremental learning, effective feature extraction, and adaptability to complex patterns. However, they require significant computational resources and large volumes of training data, and they may be prone to overfitting. (6) Combination learner methods integrate multiple approaches to leverage their strengths. While potentially powerful, they can introduce additional complexity that complicates deployment and may reduce operational performance. In this project, soft computing methods are adopted due to their ability to overcome many of the limitations of earlier methods while maintaining relative simplicity. The drawbacks they present, such as high computational demand and the need for large-scale data aggregation, are the kinds of challenges that federated learning is designed to address.

Among soft computing approaches, autoencoders have attracted growing interest in anomaly detection. As reported by \cite{9439459}, autoencoder-based methods appeared in 17 research studies between 2000 and 2020, ranking just behind SVM-based classification models and clustering approaches in frequency of use. First introduced by \cite{10.5555/104279.104293}, autoencoder is a type of unsupervised neural network model designed to learn an "informative" representation of unlabeled input data and then reconstruct this data to its original form \citep{michelucci2022introductionautoencoders}. It typically uses a symmetric architecture, which consists of an encoder, a latent representation (bottleneck), and a decoder (Figure \ref{fig:autoencoder_architecture}). Generally, the encoding and decoding processes can be expressed as:

\begin{align}
\mathbf{h}_i = g(\mathbf{x}_i), \quad \tilde{\mathbf{x}}_i = f(\mathbf{h}_i) = f(g(\mathbf{x}_i)),
\end{align}

Where $\mathbf{x}_i$ is an input sample, $\mathbf{h}_i$ is the latent representation produced by the encoder $g$, and $\tilde{\mathbf{x}}_i$ is the reconstruction produced by the decoder $f$ \citep{michelucci2022introductionautoencoders}. Non-linear activation functions such as the sigmoid or ReLU are applied after each layer to capture complex patterns in the data, while reconstruction loss is commonly measured using Mean Squared Error (MSE) or Binary Cross-Entropy (BCE) loss functions \citep{michelucci2022introductionautoencoders}. With the ability of compressing data into a latent space, autoencoders are able to reduce data dimensions, which potentially helps reduce computational costs in downstream tasks such as training a classification model \citep{michelucci2022introductionautoencoders}. Especially, this mechanism also makes autoencoders effective at detecting outliers or anomalies that fail to be represented as well as produce high reconstruction errors \citep{michelucci2022introductionautoencoders}. By these properties, autoencoders are employed in this project as the core anomaly detection model, which is trained on normal traffic data and identifies DDoS traffic as anomalous.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/autoencoder.png}
    \caption{Autoencoder Architecture \citep{ibm-autoencoder}.}
    \label{fig:autoencoder_architecture}
\end{figure}

\section{Federated Learning}

As discussed in the previous section, soft computing and neural network-based approaches, such as autoencoders, often require high computational demand and large-scale data aggregation. Federated learning comes up as a solution for this problem by distributing computational effort to multiple devices. In 2016, Google first formally introduced federated learning in a research paper by \cite{mcmahan2023communication}, which primarily was created to tackle their own problems of training machine learning models with data scattered among millions of mobile devices, such as the case of Google Keyboard training a language model to suggest the next word a user might type \citep{47586}. This rich data, such as typed texts, personal photos, and speech recordings, is usually privacy sensitive and large in quantity, leading to it being difficult to be centrally aggregated \citep{mcmahan2023communication}. Federated learning was introduced with the federated averaging (FedAvg) algorithm (Algorithm \ref{alg:fedavg}).

\begin{algorithm}
\caption{\textsc{FedAvg}. The $K$ clients are indexed by $k$; $B$ is the local minibatch size, $E$ is the number of local epochs, and $\eta_l$ is the local learning rate \citep{mcmahan2023communication}.}
\label{alg:fedavg}
\begin{algorithmic}[1]
\State \textbf{Server executes:}
\State initialize $w_0$
\For{each round $t = 1, 2, \dots$}
    \State $m \gets \max(C \cdot K, 1)$
    \State $S_t \gets$ (random set of $m$ clients)
    \For{each client $k \in S_t$ \textbf{in parallel}}
        \State $w^{k}_{t+1} \gets \textsc{ClientUpdate}(k, w_t)$
    \EndFor
    \State $n \gets \sum_{k \in S_t} n_k$
    \State $w_{t+1} \gets \sum_{k \in S_t} \frac{n_k}{n} w^{k}_{t+1}$
\EndFor
\Statex
\Function{ClientUpdate}{$k, w$} \quad // \text{Run on client $k$}
    \State $\mathcal{B} \gets$ (split $\mathcal{P}_k$ into batches of size $B$)
    \For{each local epoch $i$ from $1$ to $E$}
        \For{batch $b \in \mathcal{B}$}
            \State $w \gets w - \eta_l \nabla \ell(w; b)$
        \EndFor
    \EndFor
    \State \Return $w$ to server
\EndFunction
\end{algorithmic}
\end{algorithm}

In the FedAvg algorithm, clients (or devices) share the same machine learning model and train it with their own local datasets. Updated model parameters (or weights) in each client are then aggregated and maintained at a central server \citep{mcmahan2023communication}. This process proceeds through several rounds. Before starting, the shared model's weights are initialized on the server ($w_0$). In each round ($t$), a set $S_t$ of $m$ clients is randomly selected from the total $K$ clients involved, with the rate controlled by the parameter $C = \frac{m}{K}$. The server sends the current model weights ($w_t$) to each client $k$, which are then updated in parallel in clients. Clients train the model normally as any common neural network setup. For example, Algorithm \ref{alg:fedavg} uses stochastic gradient descent (SGD) for optimization, with $B$ minibatch size, $E$ epochs,  $\eta_l$ learning rate and the loss function $\ell$, to keep the simplicity; in fact, more advanced techniques, however, can be applied, such as the Adam optimizer for faster convergence or L2 regularization to prevent overfitting \citep{mcmahan2023communication}. Updated parameters $w^{k}_{t+1}$ in each client are sent back to the server, which are then aggregated based on the data proportion $\frac{n_k}{n}$ contributed by each client, namely the averaging process. The model weights $w_{t+1}$ are now produced and ready for the next rounds.

Besides the advantages in privacy preservation and in reducing computation and communication costs, federated learning also presents notable challenges: (1) Non-IID, which is local datasets having different distributional characteristics, leading to heterogeneous data; (2) Unbalanced, which is the amount of data varying between clients; (3) Massively distributed, which is systems involving an extremely large number of clients; and (4) Limited communication, which is network constraints, such as offline clients or slow connections, reducing communication efficiency between clients and the central server \citep{mcmahan2023communication}. These issues can cause models in federated learning to (1) train and converge more slowly and, in some cases, (2) fail to generalize or drift toward specific clients. To address these challenges, various federated optimization algorithms have been developed, each introducing strategies to improve convergence speed, model generalization, and robustness under heterogeneous data and system conditions. 

\section{Federated Optimization Algorithms}

The issue of client drifting happens in FedAvg when models are trained under heterogeneous conditions, which can be due to either the variability of system characteristics (system heterogeneity) or non-identically distributed data across the network (statistical heterogeneity) \citep{fexprox}. This can be understood as models learning from some clients "more" than others, which leads to the models overfitting some clients while underfitting the others. This problem can theoretically be solved by tuning parameters, like the number of epochs $\eta_l$, in each individual client based on its local constraints. However, this approach is unrealistic, especially when the number of clients grows; hence, the local parameters are indeed fixed for every client \citep{fexprox}. A more efficient approach was proposed by \cite{fexprox}, namely FedProx, which better handles data and system heterogeneity in federated learning environments. Unlike FedAvg, which assumes relatively homogeneous and balanced client datasets, FedProx introduces a proximal term to the local objective function (Algorithm \ref{alg:fedprox}). This additional term is the difference between global and local weights controlled by the $\mu$ parameter, which constrains local model updates and prevents them from deviating excessively from the global model. By doing so, FedProx improves stability and convergence under Non-IID conditions, mitigates the impact of unbalanced data, and tolerates variability in the number of local update steps across clients \citep{fexprox}.

\begin{algorithm}
  \caption{\colorbox{lightgray}{$\textsc{FedProx}$}. $\mu$ is the proximal parameter \citep{fexprox}.}
\label{alg:fedprox}
\begin{algorithmic}[1]
\Function{ClientUpdate}{$k, w$} \quad // \text{Run on client $k$}
    \State $\mathcal{B} \gets$ (split $\mathcal{P}_k$ into batches of size $B$)
    \State $\colorbox{lightgray}{$w_{t} \gets w$}$ 
    \For{each local epoch $i$ from $1$ to $E$}
        \For{batch $b \in \mathcal{B}$}
            \State $w \gets w - \eta_l \nabla \ell(w; b) \colorbox{lightgray}{$+ \frac{\mu}{2} \lVert w - w_t \rVert^{2}$}$
        \EndFor
    \EndFor
    \State \Return $w$ to server
\EndFunction
\end{algorithmic}
\end{algorithm}

While FedProx addresses heterogeneity by stabilizing local updates, it does not directly optimize the efficiency of gradient-based learning across diverse clients. To further accelerate convergence and adapt to varying gradient information, a family of adaptive federated optimization algorithms (FedOptim) has been proposed by \cite{fedoptim}, including FedAdagrad, FedAdam, and FedYogi. These algorithms increase the convergence speed by making use of the momentum and the second moment of gradients on the server aggregation (Algorithm \ref{alg:fedoptim}). This mechanism was inspired by notable successful optimizers, which are renowned for the ease of tuning and ability to combat unfavorable convergence behavior in non-federated settings, including Adagrad, Adam, and Yogi \citep{fedoptim}. Making use of gradient momentum is not new; \cite{fedavgm}, in a paper measuring the effects of non-IID data for federated visual classification, applied it for the first time to FedAvg, which updates weights on the server by momentum rather than the origin averaging approach. This results in an algorithm called FedAvgM:

\begin{align}
m_t \gets \beta m_{t-1} + \Delta_t, w_{t+1} \gets w_t - m_t
\end{align}

Where $\Delta_t$ is the total client updates at round $t$, $m_{t}$ is the momentum at round $t$, and $\beta \in [0, 1)$ is a parameter controlling the momentum effect. FedAvgM was demonstrated to improve classification performance under multiple non-IID conditions \citep{fedavgm}. FedOptim algorithms make this improvement further by using the gradient second momentum ($v_t$), and they, especially FedAdam and FedYogi, indeed outperformed FedAvgM in most of the tests in both the model performance and the ease of tuning \citep{fedoptim}. Algorithm \ref{alg:fedoptim} highlights all modifications that FedOptim algorithms make to the original FedAvg, including client updates aggregation ($\Delta_t$), momentum ($m_t$) and second moment ($v_t$) calculations, and updating weights ($w_{t+1}$).

\begin{algorithm}
  \caption{\colorbox{myyellow}{$\textsc{FedOptim}$}: \colorbox{myred}{$\textsc{FedAdagrad}$}, \colorbox{mygreen}{$\textsc{FedYogi}$}, and \colorbox{myblue}{$\textsc{FedAdam}$}. $\beta_1, \beta_2 \in [0, 1)$ are decay parameters, $\tau$ is the parameter controlling the degree of adaptability, and $\eta$ is the server learning rate \citep{fedoptim}.}
\label{alg:fedoptim}
\begin{algorithmic}[1]
\State \textbf{Server executes:}
\State initialize $w_0$
\For{each round $t = 1, 2, \dots$}
    \State $m \gets \max(C \cdot K, 1)$
    \State $S_t \gets$ (random set of $m$ clients)
    \For{each client $k \in S_t$ \textbf{in parallel}}
        \State $w^{k}_{t+1} \gets \textsc{ClientUpdate}(k, w_t)$
        \State $\colorbox{myyellow}{$\Delta^{k}_{t} \gets w^{k}_{t+1} - w_t$}$ 
    \EndFor
    \State $n \gets \sum_{k \in S_t} n_k$
    \State $\colorbox{myyellow}{$\Delta_{t} \gets \sum_{k \in S_t} \frac{n_k}{n} \Delta^{k}_{t}$}$
    \State $\colorbox{myyellow}{$m_{t} \gets \beta_1 m_{t-1} + (1 - \beta_1) \Delta_t$}$
    \State $\colorbox{myred}{$v_t \gets v_{t-1} + \Delta_t^2$}$
    \State $\colorbox{mygreen}{$v_t \gets v_{t-1} - (1 - \beta_2) \Delta_t^2 \operatorname{sign}(v_{t-1} - \Delta_t^2)$}$
    \State $\colorbox{myblue}{$v_t \gets \beta_2 v_{t-1} + (1 - \beta_2) \Delta_t^2$}$
    \State $w_{t+1} \gets \colorbox{myyellow}{$w_{t} + \eta \frac{m_t}{\sqrt{v_t} + \tau}$}$
\EndFor
\end{algorithmic}
\end{algorithm}

Overall, these federated optimization algorithms enhance the robustness and convergence of training under heterogeneous and distributed conditions while maintaining the same communication costs as FedAvg. By these advantages, they are promising for resource-constrained applications such as federated DDoS detection, where fast adaptation and efficient collaboration across clients are critical.

\section{Related Works}

Autoencoders have been extensively explored as a core technique in anomaly-based DDoS detection, with the ability of learning normal traffic and identify deviations of attacks. This integration of autoencoders into network intrusion detection is not new but has gained significant traction in recent years due to their effectiveness and computational efficiency. For example,  in the work of \cite{9110372}, the authors evaluated traditional autoencoders on two widely used benchmark datasets, CIC-IDS2017 and MAWI, which include both benign traffic and updated malware traces. By using a relatively simple autoencoder with the structure $[27 \rightarrow 24 \rightarrow 16 \rightarrow 24 \rightarrow 27]$, they achieved more than 94\% accuracy. This demonstrates that even without complex neural network architectures, autoencoders can provide an effective balance between detection capability and computational cost, which makes them suitable for real-world deployment in large-scale networks.

From the perspective of integrating DDoS detection with federated learning, an increasing trend has also been observed. For instance, \cite{9816297} explored a federated learning framework that combined federated averaging (FedAvg) with a convolutional neural network (CNN) to detect DDoS attacks. Their experiments conducted on the CIC-DDoS2019 dataset, which is one of the most comprehensive DDoS datasets. The authors simulated a federated setup with 50 clients and 1000 training rounds, which reflected a relatively large network. The final results were remarkable, with 99.11\% accuracy for the binary classification task and 89.69\% for the multiclass classification task. While these results are impressive, the inherent weakness of this approach is relying on a CNN, which is fundamentally a supervised model, leading to this approach being hard to deploy to real-world systems where labeled data is often insufficient. Furthermore, the study did not clearly disclose how data was partitioned across clients, making it difficult to evaluate the level of non-IIDness, which is a critical factor in federated learning performance and generalization.

In another research, \cite{9724339} proposed a more sophisticated approach, which combined an autoencoder, a recurrent neural network (RNN), and FedAvg. In this approach, the autoencoder was first used to compress input features and reduce dimensionality, after which the compressed representation was passed to the RNN for supervised training. Their experiments spanned both the CIC-IDS2017 and CIC-DDoS2019 datasets, enabling the evaluation of the model on traffic with different characteristics and attack types. The system achieved strong performance with 95.76\% accuracy on the multiclass classification task. This outcome suggests that integrating autoencoders can enhance feature representation and improve classification efficiency. However, the approach still suffers from the supervised learning constraint, and its applicability in real-world deployment is limited when labels are scarce. Similar to the previous study, the method of data partitioning among clients was not explicitly discussed, leaving uncertainty about how the system would perform under heterogeneous and non-IID conditions, which are the norm in distributed environments.

The most relevant research that directly integrates autoencoders with federated learning for DDoS detection is presented by \cite{IDRISSI2023121000}.In this work, the authors explored not only traditional autoencoders but also more advanced variants, including variational autoencoders (VAE) and adversarial autoencoders (AAE). The VAE map input to a distribution of mean $\mu$ and standard deviation $\sigma$ instead of a fixed latent space. The AAE, on the other hand, is a hybrid model that combines autoencoders and generative adversarial networks (GAN). In addition, the study also incorporated the FedProx optimization algorithm to reduce heterogeneous effects and increase model accuracy. After training on three datasets, including USTC-TFC2016, CIC-IDS2017, and CSE-CIC-IDS2018, the results returned were really promising, achieving 99.94\% accuracy on USTC-TFC2016 with the AAE model, 92.73\% on CIC-IDS2017 with the standard AE, and 90.64\% on CSE-CIC-IDS2018 with the VAE. These findings suggest that no single autoencoder variant consistently outperforms the others across different datasets. However, the research's experiments had some limitations. The federated setup was restricted to only 10 clients, which represents a relatively small-scale deployment compared to real-world distributed networks. Moreover, the client data was equally partitioned, which simulated an IID environment rather than the more challenging non-IID scenarios of federated learning. Consequently, while the results demonstrate strong potential, the conclusions underscore a significant gap in evaluating performance under heterogeneous settings.

\section{Aims \& Objectives}

The primary aim of this project is to investigate how federated learning integrated with autoencoders can be applied for anomaly-based DDoS detection in distributed network environments. Traditional centralized training often struggles with privacy concerns and scalability issues, making it unsuitable for sensitive and distributed contexts such as network intrusion detection. Federated learning offers a promising solution by enabling collaborative model training across distributed clients without directly sharing raw traffic data. However, this approach introduces its own challenges, which most notably is the issue of non-identically and independently distributed (non-IID) data. In the context of DDoS detection, traffic patterns vary widely across networks, leading to significant heterogeneity that can degrade model convergence, detection accuracy, and stability. To mitigate these challenges, this project explores advanced federated optimization algorithms, such as FedProx, FedAdam, and FedYogi, each of which has been designed to improve stability and convergence in heterogeneous environments.

To achieve this aim, the project will pursue the following objectives:

\begin{itemize}
    \item \textbf{Develop and evaluate an autoencoder for DDoS detection.} Using benchmark datasets such as CIC-IDS2017 and CIC-IDS2019, the project will design and train an autoencoder to capture the patterns of normal network traffic and identify DDoS attacks. Various threshold selection methods will be proposed to determine when deviations should be classified as anomalies. Performance metrics such as accuracy and F1-score will be measured and compared to prior research. In addition, hyperparameters such as learning rate, batch size, and regularization strength will be carefully tuned. These tuned parameters will then form the baseline configuration for subsequent federated experiments.
    \item \textbf{Investigate the performance of FedAvg under non-IID conditions.} Based on the centralized baseline, the autoencoder will be trained under federated settings with the standard FedAvg algorithm. Multiple data partitioning strategies and numbers of clients will be experimented with to simulate varying degrees of non-IIDness. Model performance will be recorded using the same metrics as in the centralized setting, which will also be compared to prior research. Moreover, the model's convergence behavior and performance will be assessed across multiple heterogeneous conditions.
    \item \textbf{Apply advanced federated optimization algorithms.} To address the shortcomings of FedAvg in heterogeneous environments, the study will implement federated optimization algorithms. Except for FedProx, three FedOptim algorithms, including FedAdagrad, FedAdam, and FedYogi, will be compared in terms of convergence behavior and model performance to select the most suitable one. Performance metrics will also be recorded and compared with FedAvg. If possible, hybrid approaches combining multiple algorithms will be explored to achieve further gains.
\end{itemize}

