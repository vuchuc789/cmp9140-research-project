This chapter describes the implementation of the proposed framework for federated learning-based DDoS detection. Unlike the methodology chapter, which outlined the high-level design, evaluation strategy, and justification of choices, this section focuses on the technical realization of the system. It introduces the overall framework diagram to illustrate the two main functional flows, federated training and anomaly detection, and explains how these flows are integrated into the architecture. The emphasis is on the toolsets, environments, design decisions, integration steps, and practical considerations that enabled the experimentation. In addition, challenges encountered during development and execution are acknowledged, providing transparency in the research process.

\section{Framework Architecture}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/framework-architechture.png}
    \caption{Framework Architecture.}
    \label{fig:framework_architecture}
\end{figure}

The overall framework of the proposed system is illustrated in Figure \ref{fig:framework_architecture}. It integrates two main functional flows: federated training of models across distributed clients and real-time anomaly detection with alert generation on each client. At the center of the framework are the clients, which represent network entities (e.g., network routers, reverse proxies, and API gateways) that both participate in federated training and monitor incoming traffic. The clients receive incoming network traffic streams, where the deployed anomaly detection model inspects packets and flows. If anomalous behavior is detected, clients either block malicious traffic or generate alerts while allowing benign traffic to pass through to the inner systems. This ensures that local security operations can be conducted immediately, without waiting for global coordination. Above the clients is the global server, which coordinates the federated learning process. The server periodically sends the global model to all participating clients. Each client uses this model as a starting point, performs local training on its private network data, and then produces updated models. These updates are sent back to the server, where they are aggregated to refine the global model. The process is repeated iteratively across multiple rounds until convergence is reached. Figure \ref{fig:framework_architecture} highlights these two flows:

\begin{itemize}
    \item \textbf{Federated Training Flow:} exchange of the global model from the server to clients, local updates from clients back to the server, and aggregation into an improved global model.
    \item \textbf{Anomaly Detection Flow:} inspection of incoming traffic at clients, filtering or alerting anomalous traffic, and forwarding only benign traffic to internal systems.
\end{itemize}

By combining these two flows, the framework ensures that detection models are continuously improved through collaborative training while remaining privacy-preserving, since raw traffic data never leaves the clients.

\section{Tools \& Environments}

The implementation leverages a combination of programming frameworks, machine learning libraries, and federated learning toolkits. Python 3.12 \citep{python} serves as the primary development language, chosen for its extensive ecosystem supporting machine learning (PyTorch, TensorFlow), data manipulation (Pandas, NumPy), and federated learning (Flower). PyTorch \citep{pytorch} is employed for model development due to its flexibility, computational efficiency, native GPU support, and widespread adoption in both academia and industry. NumPy \citep{numpy} provides the backbone for numerical computation, offering efficient array operations, while Pandas \citep{pandas} and Scikit-learn \citep{sklearn} handle data preprocessing, encoding, and dataset partitioning. Scipy \citep{scipy} is used to perform statistical analyses, including hypothesis testing, which supports the evaluation of experimental results. For visualization, Matplotlib \citep{matplotlib} and Seaborn \citep{seaborn} are adopted to generate plots and distributions that support understanding feature behavior and model performance. Flower \citep{flower} is chosen as the federated learning framework due to its maturity, modularity, and built-in simulation features with flexible data partitioning strategies. Experiments are executed on a MacBook Air M2 equipped with an 8-core CPU, 16 GB of RAM, and an 8-core integrated GPU. Source code is managed using GitHub for version control.

\section{Data Preparation \& Preprocessing}

Datasets, including CIC-IDS2017 \citep{cicids2017} and CIC-DDoS2019 \citep{cicddos2019}, are downloaded directly from the Canadian Institute for Cybersecurity's official website. Each dataset is provided as a compressed archive containing multiple CSV files, where traffic records of both benign and attack flows are mixed. These CSV files are imported using Pandas, which also handles preprocessing steps described earlier, including removing duplicate and missing records, dropping unused features, and relabeling traffic classes into two categories: \textbf{"benign"} and \textbf{"anomalous"}. Processed data is then stored in Parquet format and compressed with Zstandard (zst), which reduces storage requirements and accelerates later retrievals. Subsequently, the data undergoes statistical exploration and analysis. This includes (1) enumerating all available features, (2) examining class distributions to assess the benign-to-anomalous ratio, (3) visualizing feature distributions (with Matplotlib and Seaborn), and (4) applying inferential statistical tests for each numeric feature. Specifically, if feature distributions follow normality, a two-sample t-test is performed using Scipy’s ttest\_ind \citep{scipytt}; otherwise, the Mann–Whitney U test is applied using Scipy’s mannwhitneyu \citep{scipymt}. A significance level of $\alpha = 0.05$ is adopted for both tests. Since models are trained only on benign traffic, large volumes of benign data are downsampled to approximately 100,000 samples to align with hardware constraints. The downsampled benign data is then divided into training and testing subsets using Scikit-learn's train\_test\_split \citep{sklearntts} with an 80:20 ratio. To mitigate class imbalance, anomalous samples are also downsampled to match the size of the benign testing set. Finally, all features are normalized according to the preprocessing steps described earlier (log scaling, z-score scaling, min–max scaling, and one-hot encoding) using utilities of NumPy, Scipy, and Scikit-learn. After preprocessing, each dataset is divided into three subsets with a 80:20:20 ratio: a benign training set, a benign testing set, and an anomalous testing set. All feature values are normalized to the [0,1] range to ensure consistency across models.

\section{Autoencoder Training}

The autoencoder is designed with a symmetric architecture (79-64-32-16-32-64-79), where the input and output layers correspond to the 79 normalized features in the dataset. The encoder progressively reduces the dimensionality, with the bottleneck layer of 16 neurons serving as a compressed representation of benign traffic. The bottleneck size is set to 16 neurons, which balances information loss and compression ability. A smaller bottleneck (e.g., 8 neurons) overly restricts the model, leading to excessive information loss and poor reconstruction of benign traffic. Conversely, a larger bottleneck (e.g., 32 neurons) allows the model to reconstruct both benign and anomalous traffic with similar accuracy, reducing its discriminative power. The 16-dimensional latent space provides the optimal compromise, retaining essential traffic characteristics while enforcing sufficient compression to highlight deviations in attack traffic. Moreover, this size keeps the model lightweight, which is an important consideration for scalability in federated learning scenarios. ReLU activation functions ($f(x) = \max(0, x)$) are put between layers (except before the last layer) to introduce non-linearity to the model. A Sigmoid function ($f(x) = \frac{1}{1 + e^{-x}}$) is used before the last layer or the output layer to keep the output in the [0,1] range, which is consistent with the input. The loss function used is the MSE ($\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2$), which measures the average squared difference between input and output (reconstruction loss). The model is trained with the Stochastic Gradient Descent with Momentum (SGDM) optimizer with the default $\beta$ of 0.9 to accelerate convergence. Hyperparameters, including batch size, learning rate ($\eta_l$), and L2 regularization rate, are tested with $\{32, 64, 128\}$, $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$, and $\{10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\}$, respectively, using the random search method. Random search is a hyperparameter tuning method, which samples from predefined candidate values rather than exhaustively testing all combinations, which makes the process more efficient while still exploring diverse configurations. All of these works are implemented with PyTorch. PyTorch is able to accelerate matrix computation during model training by utilizing the integrated GPU through the Metal Performance Shaders (MPS) backend \citep{mps}. Model weights and data batches are transferred to GPU memory for computation. Note that Apple's M2 chip integrates CPU and GPU cores on the same physical memory (unified RAM), but PyTorch with MPS still performs data copying between CPU memory and GPU memory. This copying action can introduce a significant latency to the training process. After training, the model is evaluated using performance metrics, including accuracy, precision, recall, F1-score, FPR, and AUC. Thresholds for anomaly detection are determined by scikit-learn’s roc\_curve \citep{sklearnrc} and precision\_recall\_curve \citep{sklearnprc} functions, and the metrics are calculated based on the resulting confusion metric of each threshold. The best threshold and corresponding metrics for evaluation are selected based on the best F1-score, as mentioned earlier.

\section{Federated Learning}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/flower_fl.png}
    \caption{Flower Architecture \citep{flower-architecture}.}
    \label{fig:flower_fl}
\end{figure}

For federated learning, data is partitioned using Flower's IidPartitioner \citep{iid_partitioner} and DirichletPartitioner \citep{dirichlet_partitioner} ($\text{Dir}(\alpha)$) with $\alpha$ in $\{0.6, 0.3\}$ across 10, 20, and 50 clients, which results in 9 unique non-IID setups. The model is then trained with Flower's simulation engine \citep{flower-simulation}. The federated learning architecture in Flower is described in Figure \ref{fig:flower_fl}, which includes one server and several clients. The model is trained simultaneously in the ClientApps on clients, while the model aggregation is performed in the ServerApp on the server. As training and aggregation are short-term tasks, Flower implements SuperLink on servers and SuperNode on clients as long-running processes, which use gRPC for client-server communication. In the simulation environment, the main process, which runs the simulation, plays the role of a server running the ServerApp, while worker processes are spawned to run ClientApps. The default configuration of Flower allocates each worker process 2 CPU cores, so with an 8-core CPU, there are 4 workers running in parallel. The simulation mode uses in-memory communication instead of gRPC. As previously noted, these architectural and operational differences make it impossible to evaluate communication overhead, network latency, and client disconnections in real-world deployments. Training is conducted over 10, 20, and 30 rounds for 10-, 20-, and 50-client setups, respectively, each with 10 local epochs ($E$) per client. To reflect realistic participation scenarios, the fraction of participating clients ($C$) is set to 0.5, 0.25, and 0.1 for 10, 20, and 50 clients, respectively. These configurations balance convergence requirements with hardware constraints and form the baseline setup for subsequent federated learning experiments. On top of this setup, the FedAvg, FedProx, and FedOptim algorithms are subsequently experimented with. FedProx modifies ClientApps by adding the proximal term ($\frac{\mu}{2} \lVert w - w_t \rVert^{2}$), with $\mu$ in $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\}$, to the local loss functions, while FedOptim algorithms (FedAdagrad, FedAdam, and FedYogi) modify the ServerApp by applying momentum-based optimizers with default $\beta_1$, $\beta_2$, $\tau$, and a server learning rate $\eta$ in $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$. Performance metric calculation and random search strategy are still applied as described previously. Due to the large number of performance metrics produced, the paired t-test can be performed with Scipy's ttest\_rel \citep{scipyttp} to evaluate the performance differences between algorithms.

In summary, this chapter has detailed the implementation of the proposed federated learning framework, from system architecture and preprocessing to autoencoder training and federated optimization. The core model is implemented in PyTorch, integrated with Flower for federated experimentation, and supported by additional Python libraries. All experiments are executed on an Apple MacBook Air M2, with considerations for multiprocessing and hardware constraints. The full implementation and related documentation are available in the project’s GitHub repository: \url{https://github.com/vuchuc789/cmp9140-research-project}.
