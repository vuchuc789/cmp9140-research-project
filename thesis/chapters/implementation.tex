This chapter describes the implementation of the proposed framework for federated learning-based DDoS detection. Unlike the methodology chapter, which outlined the high-level design, evaluation strategy, and justification of choices, this section focuses on the technical aspects of the system. The emphasis is on the toolsets, environments, design decisions, integration steps, and practical considerations that enabled the experimentation. In addition, challenges encountered during development and execution are acknowledged, providing transparency in the research process.

The implementation leverages a combination of programming frameworks, machine learning libraries, and federated learning toolkits. Python 3.12 \citep{python} serves as the primary development language, chosen for its extensive ecosystem supporting machine learning (PyTorch, TensorFlow), data manipulation (Pandas, NumPy), and federated learning (Flower). PyTorch \citep{pytorch} is employed for model development due to its flexibility, computational efficiency, native GPU support, and widespread adoption in both academia and industry. NumPy \citep{numpy} provides the backbone for numerical computation, offering efficient array operations, while Pandas \citep{pandas} and Scikit-learn \citep{sklearn} handle data preprocessing, encoding, and dataset partitioning. SciPy \citep{scipy} is used to perform statistical analyses, including hypothesis testing, which supports the evaluation of experimental results. For visualization, Matplotlib \citep{matplotlib} and Seaborn \citep{seaborn} are adopted to generate plots and distributions that support understanding feature behavior and model performance. Flower \citep{flower} is chosen as the federated learning framework due to its maturity, modularity, and built-in simulation features with flexible data partitioning strategies. Experiments are executed on a MacBook Air M2 equipped with an 8-core CPU, 16 GB of RAM, and an 8-core integrated GPU. Source code is managed using GitHub for version control.

Datasets, including CIC-IDS2017 \citep{cicids2017} and CIC-DDoS2019 \citep{cicddos2019}, are downloaded directly from the Canadian Institute for Cybersecurity's official website. Each dataset is provided as a compressed archive containing multiple CSV files, where traffic records of both benign and attack flows are mixed. These CSV files are imported using Pandas, which also handles preprocessing steps described earlier, including removing duplicate and missing records, dropping unused features, and relabeling traffic classes into two categories: \textbf{"benign"} and \textbf{"anomalous"}. Processed data is then stored in Parquet format and compressed with Zstandard (zst), which reduces storage requirements and accelerates later retrievals. Subsequently, the data undergoes statistical exploration and analysis. This includes (1) enumerating all available features, (2) examining class distributions to assess the benign-to-anomalous ratio, (3) visualizing feature distributions (with Matplotlib and Seaborn), and (4) applying inferential statistical tests for each numeric feature. Specifically, if feature distributions follow normality, a two-sample t-test is performed using SciPy’s ttest\_ind \citep{scipytt}; otherwise, the Mann–Whitney U test is applied using SciPy’s mannwhitneyu \citep{scipymt}. A significance level of $\alpha = 0.05$ is adopted for both tests. Since models are trained only on benign traffic, large volumes of benign data are downsampled to approximately 100,000 samples to align with hardware constraints. The downsampled benign data is then divided into training and testing subsets using Scikit-learn's train\_test\_split \citep{sklearntts} with an 80:20 ratio. To mitigate class imbalance, anomalous samples are also downsampled to match the size of the benign testing set. Finally, all features are normalized according to the preprocessing steps described earlier (log scaling, z-score scaling, min–max scaling, and one-hot encoding) using utilities of NumPy, SciPy, and Scikit-learn. After preprocessing, each dataset is divided into three subsets with a 80:20:20 ratio: a benign training set, a benign testing set, and an anomalous testing set. All feature values are normalized to the [0,1] range to ensure consistency across models.

The autoencoder is designed with a symmetric architecture (79-64-32-16-32-64-79), where the input and output layers correspond to the 79 normalized features in the dataset. The encoder progressively reduces the dimensionality, with the bottleneck layer of 16 neurons serving as a compressed representation of benign traffic. The bottleneck size is set to 16 neurons, which balances information loss and compression ability. A smaller bottleneck (e.g., 8 neurons) overly restricts the model, leading to excessive information loss and poor reconstruction of benign traffic. Conversely, a larger bottleneck (e.g., 32 neurons) allows the model to reconstruct both benign and anomalous traffic with similar accuracy, reducing its discriminative power. The 16-dimensional latent space provides the optimal compromise, retaining essential traffic characteristics while enforcing sufficient compression to highlight deviations in attack traffic. Moreover, this size keeps the model lightweight, which is an important consideration for scalability in federated learning scenarios. ReLU activation functions ($f(x) = \max(0, x)$) are put between layers (except before the last layer) to introduce non-linearity to the model. A Sigmoid function ($f(x) = \frac{1}{1 + e^{-x}}$) is used before the last layer or the output layer to keep the output in the [0,1] range, which is consistent with the input. The loss function used is the MSE ($\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2$), which measures the average squared difference between input and output (reconstruction loss). The model is trained with the Stochastic Gradient Descent with Momentum (SGDm) optimizer with the default $\beta$ of 0.9 to accelerate convergence. Hyperparameters, including batch size, learning rate ($\eta_l$), and L2 regularization rate, are tested with $\{32, 64, 128\}$, $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$, and $\{10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\}$, respectively, using the random search method. Random search is a hyperparameter tuning method, which samples from predefined candidate values rather than exhaustively testing all combinations, which makes the process more efficient while still exploring diverse configurations. All of these works are implemented with PyTorch. PyTorch is able to accelerate matrix computation during model training by utilizing the integrated GPU through the Metal Performance Shaders (MPS) backend \citep{mps}. Model weights and data batches are transferred to GPU memory for computation. Note that Apple's M2 chip integrates CPU and GPU cores on the same physical memory (unified RAM), but PyTorch with MPS still performs data copying between CPU memory and GPU memory. This copying action can introduce a significant latency to the training process. After training, the model is evaluated using performance metrics, including accuracy, precision, recall, F1-score, FPR, and AUC. Thresholds for anomaly detection are determined by scikit-learn’s roc\_curve \citep{sklearnrc} and precision\_recall\_curve \citep{sklearnprc} functions, and the metrics are calculated based on the resulting confusion metric of each threshold. The best threshold and corresponding metrics for evaluation are selected based on the best F1-score, as mentioned earlier.

For federated learning, data is partitioned using Flower's IidPartitioner and DirichletPartitioner ($\text{Dir}(\alpha)$ with $\alpha \in \{0.6, 0.3\}$) across 10, 20, and 50 clients, which results in 9 unique non-IID setups. The model is then trained with Flower's simulation engine. The federated learning architecture in Flower is described in Figure \ref{fig:flower_fl}, which includes one server and several clients. The model is trained simultaneously in the ClientApps on clients, while the model aggregation is performed in the ServerApp on the server. As training and aggregation are short-term tasks, Flower implements SuperLink on servers and SuperNode on clients as long-running processes, which use gRPC for client-server communication. In the simulation environment, the main process, which runs the simulation, plays the role of a server running the ServerApp, while worker processes are spawned to run ClientApps. The default configuration of Flower allocates each worker process 2 CPU cores, so with an 8-core CPU, there are 4 workers running in parallel. The simulation mode uses in-memory communication instead of gRPC. Due to these differences in design and mechanism, it is again unable to assess communication overhead, network latency, and client dropouts in real-world deployments. Training is conducted over 10, 20, and 30 rounds for 10-, 20-, and 50-client setups, respectively, each with 10 local epochs ($E$) per client. To reflect realistic participation scenarios, the fraction of participating clients ($C$) is set to 0.5, 0.25, and 0.1 for 10, 20, and 50 clients, respectively. These configurations balance convergence requirements with hardware constraints and form the baseline setup for subsequent federated learning experiments. On top of this setup, the FedAvg, FedProx, and FedOptim algorithms are subsequently experimented with. FedProx modifies ClientApps by adding the proximal term ($\frac{\mu}{2} \lVert w - w_t \rVert^{2}$), with $\mu$ in $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\}$, to the local loss functions, while FedOptim algorithms (FedAdagrad, FedAdam, and FedYogi) modify the ServerApp by applying momentum-based optimizers with default $\beta_1$, $\beta_2$, $\tau$, and a server learning rate $\eta$ in $\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$. Performance metric calculation and random search strategy are still applied as described previously.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/flower_fl.png}
    \caption{Flower Architecture \citep{flower-architecture}.}
    \label{fig:flower_fl}
\end{figure}

In summary, the project’s core model is implemented in PyTorch, trained in a federated setting with Flower, and evaluated using supporting Python libraries. All experiments are conducted on an Apple MacBook Air M2, with considerations for CPU–GPU data transfer and the multiprocessing simulation environment. The complete implementation and related documentation are published in the GitHub repository: \url{https://github.com/vuchuc789/cmp9140-research-project}.
